{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "#import pickle\n",
    "#import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "profiles_data = pd.read_csv('profiles.csv')\n",
    "\n",
    "train_set = pd.read_csv(\"train.csv\", index_col=[0,1])\n",
    "train_set_unstacked = train_set.unstack(level=-1)\n",
    "train_set_unstacked.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_KM = train\n",
    "train_R = train\n",
    "\n",
    "train = train_set_unstacked['plays']\n",
    "train.loc[:,'user'] = train.index\n",
    "train.index.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users missing age: 44842\n",
      "number of users missing sex: 19535\n",
      "number of users missing country: 0\n",
      "number of users with negative age: 40\n",
      "number of users with high age: 4\n"
     ]
    }
   ],
   "source": [
    "print \"number of users missing age: %s\" % sum(profiles_data['age'].isnull())\n",
    "print \"number of users missing sex: %s\" % sum(profiles_data['sex'].isnull())\n",
    "print \"number of users missing country: %s\" % sum(profiles_data['country'].isnull())\n",
    "print \"number of users with negative age: %s\" % sum(profiles_data['age'] < 0 )\n",
    "print \"number of users with high age: %s\" % sum(profiles_data['age'] > 120 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat([profiles_data, pd.get_dummies(profiles_data['sex'], dummy_na=True)], axis=1)\n",
    "df.rename(columns = lambda x: str(x), inplace=True)\n",
    "df.rename(columns = {'nan': 'sex-nan'}, inplace=True)\n",
    "del df['sex']\n",
    "\n",
    "df['age-nan'] = np.isnan(df['age'])*1\n",
    "nans = np.isnan(df['age'])\n",
    "df.loc[nans,'age'] = 0\n",
    "df['age-negative'] = (df['age']<0)*1\n",
    "negatives = df['age-negative']==1\n",
    "df.loc[negatives,'age'] = 0\n",
    "df['age-toohigh'] = (df['age']>120)*1\n",
    "toohighs = df['age-toohigh']==1\n",
    "df.loc[toohighs,'age'] = 0\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(df['country'], dummy_na=False)], axis=1)\n",
    "del df['country']\n",
    "\n",
    "df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000d90ec-d64c-48a1-b775-e726fd240e9f</th>\n",
       "      <th>000fc734-b7e1-4a01-92d1-f544261b43f5</th>\n",
       "      <th>0019749d-ee29-4a5f-ab17-6bfa11deb969</th>\n",
       "      <th>0039c7ae-e1a7-4a7d-9b49-0cbc716821a6</th>\n",
       "      <th>004e5eed-e267-46ea-b504-54526f1f377d</th>\n",
       "      <th>00565b31-14a3-4913-bd22-385eb40dd13c</th>\n",
       "      <th>00a9f935-ba93-4fc8-a33a-993abe9c936b</th>\n",
       "      <th>00eeed6b-5897-4359-8347-b8cd28375331</th>\n",
       "      <th>0103c1cc-4a09-4a5d-a344-56ad99a77193</th>\n",
       "      <th>0110e63e-0a9b-4818-af8e-41e180c20b9a</th>\n",
       "      <th>...</th>\n",
       "      <th>Vanuatu</th>\n",
       "      <th>Venezuela</th>\n",
       "      <th>Viet Nam</th>\n",
       "      <th>Virgin Islands, British</th>\n",
       "      <th>Virgin Islands, U.s.</th>\n",
       "      <th>Wallis and Futuna</th>\n",
       "      <th>Western Sahara</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Zambia</th>\n",
       "      <th>Zimbabwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00000c289a1829a808ac09c00daf10bc3c4e223b</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>   0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001411dc427966b17297bf4d69e7e193135d89</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>   0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000063d3fe1cf2ba248b9e3c3f0334845a27a6bf</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>   0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007a47085b9aab8af55f52ec8846ac479ac4fe</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 456</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000c176103e538d5c9828e695fed4f7ae42dd01</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>   0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          000d90ec-d64c-48a1-b775-e726fd240e9f  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          000fc734-b7e1-4a01-92d1-f544261b43f5  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          0019749d-ee29-4a5f-ab17-6bfa11deb969  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          0039c7ae-e1a7-4a7d-9b49-0cbc716821a6  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          004e5eed-e267-46ea-b504-54526f1f377d  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          00565b31-14a3-4913-bd22-385eb40dd13c  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          00a9f935-ba93-4fc8-a33a-993abe9c936b  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          00eeed6b-5897-4359-8347-b8cd28375331  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          0103c1cc-4a09-4a5d-a344-56ad99a77193  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                          0110e63e-0a9b-4818-af8e-41e180c20b9a  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                                   456   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                                     0   \n",
       "\n",
       "                                            ...     Vanuatu  Venezuela  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b    ...           0          0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89    ...           0          0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf    ...           0          0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe    ...           0          0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01    ...           0          0   \n",
       "\n",
       "                                          Viet Nam  Virgin Islands, British  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b         0                        0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89         0                        0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf         0                        0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe         0                        0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01         0                        0   \n",
       "\n",
       "                                          Virgin Islands, U.s.  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                     0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                     0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                     0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                     0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                     0   \n",
       "\n",
       "                                          Wallis and Futuna  Western Sahara  \\\n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b                  0               0   \n",
       "00001411dc427966b17297bf4d69e7e193135d89                  0               0   \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf                  0               0   \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe                  0               0   \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01                  0               0   \n",
       "\n",
       "                                          Yemen  Zambia  Zimbabwe  \n",
       "00000c289a1829a808ac09c00daf10bc3c4e223b      0       0         0  \n",
       "00001411dc427966b17297bf4d69e7e193135d89      0       0         0  \n",
       "000063d3fe1cf2ba248b9e3c3f0334845a27a6bf      0       0         0  \n",
       "00007a47085b9aab8af55f52ec8846ac479ac4fe      0       0         0  \n",
       "0000c176103e538d5c9828e695fed4f7ae42dd01      0       0         0  \n",
       "\n",
       "[5 rows x 2246 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.merge(train,df,on='user')\n",
    "data = data.set_index('user')\n",
    "data.index.name = None\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "Overall Mean/Median + User Bias + Artist Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation\n",
    "Narrow down parameters (1. How to deal with negative predictions, 2. Mean v. Median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n=len(train), n_folds=5, shuffle=False, random_state=3)\n",
    "\n",
    "kfold_predictions = []\n",
    "MAE = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    X_train, X_test = train.ix[train_index, :], train.ix[test_index, :]\n",
    "    \n",
    "    overall_mean = X_train['plays'].mean()\n",
    "    overall_median = X_train['plays'].median()\n",
    "\n",
    "    subdf = X_train[['artist', 'plays']]\n",
    "    #artist_means = subdf.groupby('artist').mean()\n",
    "    #artist_means['avgplays'] = artist_means['plays'] - overall_mean\n",
    "    #del artist_means['plays']\n",
    "    #artist_means = pd.DataFrame.to_dict(artist_means)['avgplays']\n",
    "    artist_medians = subdf.groupby('artist').median()\n",
    "    artist_medians['medplays'] = artist_medians['plays'] - overall_median\n",
    "    del artist_medians['plays']\n",
    "    artist_medians = pd.DataFrame.to_dict(artist_medians)['medplays']\n",
    "\n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    #user_means = subdf.groupby('user').mean()\n",
    "    #user_means['avgplays'] = user_means['plays'] - overall_mean\n",
    "    #del user_means['plays']\n",
    "    #user_means = pd.DataFrame.to_dict(user_means)['avgplays']\n",
    "    user_medians = subdf.groupby('user').median()\n",
    "    user_medians['medplays'] = user_medians['plays'] - overall_median\n",
    "    del user_medians['plays']\n",
    "    user_medians = pd.DataFrame.to_dict(user_medians)['medplays']\n",
    "    \n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    #user_means = subdf.groupby('user').mean()\n",
    "    #user_means['avgplays'] = user_means['plays'] - overall_mean\n",
    "    #del user_means['plays']\n",
    "    #user_means = pd.DataFrame.to_dict(user_means)['avgplays']\n",
    "    user_mins = subdf.groupby('user').min()\n",
    "    user_mins['medplays'] = user_mins['plays']\n",
    "    del user_mins['plays']\n",
    "    user_mins = pd.DataFrame.to_dict(user_mins)['medplays']\n",
    "    \n",
    "    predictions = []\n",
    "    for i, j in zip(X_test['user'], X_test['artist']):\n",
    "        '''\n",
    "        try:\n",
    "            p = overall_mean + user_means[i] + artist_means[j]\n",
    "            #if p < 0:\n",
    "            #    p = 0\n",
    "            predictions.append(p)\n",
    "        except:\n",
    "            p = overall_mean + 0 + artist_means[j]\n",
    "            #if p < 0:\n",
    "            #    p = 0\n",
    "            predictions.append(p)\n",
    "        '''\n",
    "        try:\n",
    "            p = overall_median + user_medians[i] + artist_medians[j]\n",
    "            if p < 0:\n",
    "                p = user_mins[i]\n",
    "            predictions.append(p)\n",
    "        except:\n",
    "            p = overall_median + 0 + artist_medians[j]\n",
    "            if p < 0:\n",
    "                p = user_mins[i]\n",
    "            predictions.append(p)\n",
    "    #shift = min(predictions)\n",
    "    #predictions = predictions + shift\n",
    "    \n",
    "    MAE.append(sum(abs(X_test['plays'] - predictions))/len(X_test))\n",
    "    kfold_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142.37319802012368, 141.16751147165752, 141.41875491148201, 140.70536042004377, 140.74433065370175]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141.28183109540174"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print MAE # MEDIAN: for shifting negative predictions to min; mean = \n",
    "np.mean(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142.73132674096618, 141.52288194030766, 141.77577226849394, 141.06299934653973, 141.10266438817752]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141.63912893689698"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print MAE # MEDIAN: for shifting negative predictions to min /2; mean = 141.63912893689698\n",
    "np.mean(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257.28789827309532,\n",
       " 258.15533086042217,\n",
       " 256.69504440356701,\n",
       " 255.94572232779623,\n",
       " 257.43756353338642]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE # MEAN: for shifting every prediciton up to min; mean = 257.10431187965344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[176.25186801713923,\n",
       " 174.94202683803979,\n",
       " 175.06217330929474,\n",
       " 175.41404116109535,\n",
       " 175.36080979232705]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE # MEAN: for setting = 0; mean = 175.40618382357923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[186.90224450967014,\n",
       " 184.03921543851035,\n",
       " 184.39845913346113,\n",
       " 184.14110168828623,\n",
       " 184.26433943872149]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE # MEDIAN: for shifting every prediciton up to min; mean = 184.74907204172987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[143.100447554097,\n",
       " 141.88868611162258,\n",
       " 142.14390928094099,\n",
       " 141.43150280217725,\n",
       " 141.47249566766149]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE # MEDIAN: for setting = 0; mean = 142.00740828329987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions using baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out test solutions - MEAN\n",
    "overall_mean = train['plays'].mean()\n",
    "\n",
    "subdf = train[['artist', 'plays']]\n",
    "artist_means = subdf.groupby('artist').mean()\n",
    "artist_means['avgplays'] = artist_means['plays'] - overall_mean\n",
    "del artist_means['plays']\n",
    "artist_means = pd.DataFrame.to_dict(artist_means)['avgplays']\n",
    "\n",
    "subdf = train[['user', 'plays']]\n",
    "user_means = subdf.groupby('user').mean()\n",
    "user_means['avgplays'] = user_means['plays'] - overall_mean\n",
    "del user_means['plays']\n",
    "user_means = pd.DataFrame.to_dict(user_means)['avgplays']\n",
    "\n",
    "predictions = []\n",
    "for i, j in zip(test['user'], test['artist']):\n",
    "    p = overall_mean + user_means[i] + artist_means[j]\n",
    "    if p < 0:\n",
    "        p = 0\n",
    "    predictions.append(p)\n",
    "\n",
    "# Save to a file named mean_adjust.csv\n",
    "soln_file  = 'mean_adjust.csv'\n",
    "test_file  = 'test.csv'\n",
    "\n",
    "with open(test_file, 'r') as test_fh:\n",
    "    test_csv = csv.reader(test_fh, delimiter=',', quotechar='\"')\n",
    "    next(test_csv, None)\n",
    "\n",
    "    with open(soln_file, 'w') as soln_fh:\n",
    "        soln_csv = csv.writer(soln_fh,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "        soln_csv.writerow(['Id', 'plays'])\n",
    "\n",
    "        for row in test_csv:\n",
    "            id     = row[0]\n",
    "            p = predictions[int(id)-1]\n",
    "\n",
    "            soln_csv.writerow([id, p])\n",
    "# KAGGLE SCORE: 174.23682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out test solutions - MEDIAN\n",
    "overall_median = train_R['plays'].median()\n",
    "\n",
    "subdf = train_R[['artist', 'plays']]\n",
    "artist_medians = subdf.groupby('artist').median()\n",
    "artist_medians['medplays'] = artist_medians['plays'] - overall_median\n",
    "del artist_medians['plays']\n",
    "artist_medians = pd.DataFrame.to_dict(artist_medians)['medplays']\n",
    "\n",
    "subdf = train_R[['user', 'plays']]\n",
    "user_medians = subdf.groupby('user').median()\n",
    "user_medians['medplays'] = user_medians['plays'] - overall_median\n",
    "del user_medians['plays']\n",
    "user_medians = pd.DataFrame.to_dict(user_medians)['medplays']\n",
    "\n",
    "subdf = train_R[['user', 'plays']]\n",
    "user_mins = subdf.groupby('user').min()\n",
    "user_mins['medplays'] = user_mins['plays']\n",
    "del user_mins['plays']\n",
    "user_mins = pd.DataFrame.to_dict(user_mins)['medplays']\n",
    "\n",
    "predictions = []\n",
    "for i, j in zip(test['user'], test['artist']):\n",
    "    p = overall_median + user_medians[i] + artist_medians[j]\n",
    "    if p < 0:\n",
    "        p = user_mins[i]\n",
    "    predictions.append(p)\n",
    "    \n",
    "    \n",
    "# Save to a file named mean_adjust.csv\n",
    "soln_file  = 'median_minshift.csv'\n",
    "test_file  = 'test.csv'\n",
    "\n",
    "with open(test_file, 'r') as test_fh:\n",
    "    test_csv = csv.reader(test_fh, delimiter=',', quotechar='\"')\n",
    "    next(test_csv, None)\n",
    "\n",
    "    with open(soln_file, 'w') as soln_fh:\n",
    "        soln_csv = csv.writer(soln_fh,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "        soln_csv.writerow(['Id', 'plays'])\n",
    "\n",
    "        for row in test_csv:\n",
    "            id     = row[0]\n",
    "            p = predictions[int(id)-1]\n",
    "\n",
    "            soln_csv.writerow([id, p])\n",
    "# KAGGLE SCORE: 141.24741\n",
    "# KAGGLE SCORE (shifting to min): 140.53659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "Cluster the users into K classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading:  https://www.codementor.io/python/tutorial/build-data-products-django-machine-learning-clustering-user-preferences\n",
    "\n",
    "# Truncate Data (dimension-reduction)\n",
    "trunc = TruncatedSVD(n_components=100, random_state=4)\n",
    "data_reduced = trunc.fit_transform(data)\n",
    "data_reduced = pd.DataFrame(data_reduced)\n",
    "\n",
    "data_KM = pd.merge(train,df,on='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster users into classes\n",
    "K = 2 # Number of classes\n",
    "kmeans = KMeans(n_clusters=K, init='random', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=3, copy_x=True, n_jobs=1)\n",
    "kmeans.fit(data_reduced)\n",
    "\n",
    "# Make dict of form User: Class\n",
    "keys = list(data.index)\n",
    "values = list(kmeans.labels_)\n",
    "labels_dict = dict(zip(keys, values))\n",
    "\n",
    "# Summary of results\n",
    "# K = 100; 68 > 1; k-means++; avg k-fold 209.08475033596352\n",
    "# K = 70; 49 > 1; k-means++; avg k-fold 207.22458452290766\n",
    "# K = 50; 32 > 1; k-means++; avg k-fold 206.91192287314456\n",
    "# K = 30; 15 > 1; k-means++; avg k-fold 203.56766892943034\n",
    "# K = 10; 5 > 1; k-means++; avg k-fold 202.09096861815152\n",
    "# K = 100; 90 > 1; random; avg k-fold 168.27485843741027\n",
    "# K = 70; 64 > 1; random; avg k-fold 155.22775993231548\n",
    "# K = 30; 26 > 1; random; avg k-fold 153.22289820225075\n",
    "# K = 10; 8 > 1; random; avg k-fold 149.80848768499814\n",
    "# K = 5; 4 > 1; random; avg k-fold 148.5971242801875\n",
    "# K = 3; 2 > 1; random; avg k-fold 147.22173171754909"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enter labels into train and test dataframes\n",
    "\n",
    "train_labels = []\n",
    "for i in train_KM['user']:\n",
    "    train_labels.append(labels_dict[i])\n",
    "train_KM['label'] = train_labels\n",
    "\n",
    "\n",
    "test_labels = [] \n",
    "for i in test['user']:\n",
    "    test_labels.append(labels_dict[i])   \n",
    "test['label'] = test_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform 5-fold cross validation\n",
    "\n",
    "kf = KFold(n=len(train), n_folds=5, shuffle=False, random_state=3)\n",
    "\n",
    "kfold_predictions = []\n",
    "MAE = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    X_train, X_test = train_KM.ix[train_index, :], train_KM.ix[test_index, :]\n",
    "    \n",
    "    # Group Medians\n",
    "    subdf = X_train[['plays', 'label']]\n",
    "    group_medians = subdf.groupby('label').median()\n",
    "    group_medians = pd.DataFrame.to_dict(group_medians)['plays']\n",
    "    \n",
    "    # User Medians\n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    user_medians = subdf.groupby('user').median()\n",
    "    user_medians = pd.DataFrame.to_dict(user_medians)['plays']\n",
    "    \n",
    "    #Artist Medians\n",
    "    subdf = X_train[['artist', 'plays']]\n",
    "    artist_medians = subdf.groupby('artist').median()\n",
    "    artist_medians = pd.DataFrame.to_dict(artist_medians)['plays']\n",
    "    \n",
    "    subdf = train_R[['user', 'plays']]\n",
    "    user_mins = subdf.groupby('user').min()\n",
    "    user_mins['medplays'] = user_mins['plays']\n",
    "    del user_mins['plays']\n",
    "    user_mins = pd.DataFrame.to_dict(user_mins)['medplays']\n",
    "    \n",
    "    predictions = []\n",
    "    for i, j, k in zip(X_test['label'], X_test['user'], X_test['artist']):\n",
    "        try:\n",
    "            p = group_medians[i] + (user_medians[j] - group_medians[i]) + (artist_medians[k] - group_medians[i])\n",
    "            if p < 0:\n",
    "                p = user_mins[i]\n",
    "            predictions.append(p)\n",
    "        except:\n",
    "            try:\n",
    "                p = group_medians[i] + 0 + (artist_medians[k] - group_medians[i])\n",
    "                if p < 0:\n",
    "                    p = user_mins[i]\n",
    "                predictions.append(p)\n",
    "            except:\n",
    "                try:\n",
    "                    p = 0 + user_medians[j] + artist_medians[k]\n",
    "                    if p < 0:\n",
    "                        p = user_mins[i]\n",
    "                    predictions.append(p)\n",
    "                except:\n",
    "                    try:\n",
    "                        p = group_medians[i] + 0 + (artist_medians[k] - group_medians[i])\n",
    "                        if p < 0:\n",
    "                            p = user_mins[i]\n",
    "                        predictions.append(p)\n",
    "                    except:\n",
    "                        p = 0 + 0 + artist_medians[k]\n",
    "                        if p < 0:\n",
    "                            p = user_mins[i]\n",
    "                        predictions.append(p)\n",
    "                    \n",
    "    MAE.append(sum(abs(X_test['plays'] - predictions))/len(X_test))\n",
    "    kfold_predictions.append(predictions)\n",
    "\n",
    "print np.mean(MAE)\n",
    "print MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out test solutions - K-means; initialization = ; K clusters = ;\n",
    "\n",
    "# Group Medians\n",
    "subdf = train_KM[['plays', 'label']]\n",
    "group_medians = subdf.groupby('label').median()\n",
    "group_medians = pd.DataFrame.to_dict(group_medians)['plays']\n",
    "\n",
    "# User Medians\n",
    "subdf = train_KM[['user', 'plays']]\n",
    "user_medians = subdf.groupby('user').median()\n",
    "user_medians = pd.DataFrame.to_dict(user_medians)['plays']\n",
    "\n",
    "#Artist Medians\n",
    "subdf = train_KM[['artist', 'plays']]\n",
    "artist_medians = subdf.groupby('artist').median()\n",
    "artist_medians = pd.DataFrame.to_dict(artist_medians)['plays']\n",
    "\n",
    "# Prediction:\n",
    "# Group Median + User Median + Artist Median\n",
    "predictions = []\n",
    "for i, j, k in zip(test['label'], test['user'], test['artist']):\n",
    "    p = group_medians[i] + (user_medians[j] - group_medians[i]) + (artist_medians[k] - group_medians[i])\n",
    "    if p < 0:\n",
    "        p = 0\n",
    "    predictions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to a file named mean_adjust.csv\n",
    "soln_file  = 'kmeans_100_random.csv'\n",
    "test_file  = 'test.csv'\n",
    "\n",
    "with open(test_file, 'r') as test_fh:\n",
    "    test_csv = csv.reader(test_fh, delimiter=',', quotechar='\"')\n",
    "    next(test_csv, None)\n",
    "\n",
    "    with open(soln_file, 'w') as soln_fh:\n",
    "        soln_csv = csv.writer(soln_fh,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "        soln_csv.writerow(['Id', 'plays'])\n",
    "\n",
    "        for row in test_csv:\n",
    "            id     = row[0]\n",
    "            p = predictions[int(id)-1]\n",
    "\n",
    "            soln_csv.writerow([id, p])\n",
    "# KAGGLE SCORE: 165.70278 (K=100, random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34923353  1.06158177]\n",
      "[ 1.35553713  1.06510331]\n",
      "[ 1.35267186  1.05914421]\n",
      "[ 1.34861051  1.07320877]\n",
      "[ 1.35259899  1.0678157 ]\n"
     ]
    }
   ],
   "source": [
    "# MEDIAN / MEAN\n",
    "# Apply min when predictions are < 0\n",
    "\n",
    "kf = KFold(n=len(train), n_folds=5, shuffle=False, random_state=3)\n",
    "kfold_predictions = []\n",
    "MAE = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    # Dataset for Regression\n",
    "    train_R = train_KM\n",
    "\n",
    "    X_train, X_test = train_R.ix[train_index, :], train_R.ix[test_index, :]\n",
    "    \n",
    "    subdf = X_train[['artist', 'plays']]\n",
    "    artist_medians = subdf.groupby('artist').median()\n",
    "    artist_medians['medplays'] = artist_medians['plays']\n",
    "    del artist_medians['plays']\n",
    "    artist_medians = pd.DataFrame.to_dict(artist_medians)['medplays']\n",
    "\n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    user_medians = subdf.groupby('user').median()\n",
    "    user_medians['medplays'] = user_medians['plays']\n",
    "    del user_medians['plays']\n",
    "    user_medians = pd.DataFrame.to_dict(user_medians)['medplays']\n",
    "    \n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    user_mins = subdf.groupby('user').min()\n",
    "    user_mins['medplays'] = user_mins['plays']\n",
    "    del user_mins['plays']\n",
    "    user_mins = pd.DataFrame.to_dict(user_mins)['medplays']\n",
    "\n",
    "    # Training Dataset\n",
    "    user_m = []\n",
    "    artist_m = []\n",
    "    user_min = []\n",
    "    for i, j in zip(X_train['user'], X_train['artist']):\n",
    "        try:\n",
    "            user_m.append(user_medians[i])\n",
    "        except:\n",
    "            user_m.append(0)\n",
    "        try:\n",
    "            user_min.append(user_mins[i])\n",
    "        except:\n",
    "            user_min.append(0)\n",
    "        artist_m.append(artist_medians[j])      \n",
    "\n",
    "    X_train['user_median'] = user_m\n",
    "    X_train['artist_median'] = artist_m\n",
    "    X_train['user_min'] = user_min\n",
    "    \n",
    "    X_train1 = X_train\n",
    "\n",
    "\n",
    "    # Test Dataset\n",
    "    user_m = []\n",
    "    artist_m = []\n",
    "    user_min = []\n",
    "    for i, j in zip(X_test['user'], X_test['artist']):\n",
    "        try:\n",
    "            user_m.append(user_medians[i])\n",
    "        except:\n",
    "            user_m.append(0)\n",
    "        try:\n",
    "            user_min.append(user_mins[i])\n",
    "        except:\n",
    "            user_min.append(0)\n",
    "        artist_m.append(artist_medians[j])\n",
    "\n",
    "    X_test['user_median'] = user_m\n",
    "    X_test['artist_median'] = artist_m\n",
    "    X_test['user_min'] = user_min\n",
    "    \n",
    "    X_test1 = X_test\n",
    "\n",
    "    # Make X_train, X_test, Y_train\n",
    "    Y_train = X_train['plays']\n",
    "    X_train = X_train[['user_median', 'artist_median']]\n",
    "    Y_test = X_test['plays']\n",
    "    X_test = X_test[['user_median', 'artist_median']]\n",
    "\n",
    "    # Predictions\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, Y_train)\n",
    "    predictions = regr.predict(X_test)\n",
    "    \n",
    "    for p in range(len(predictions)):\n",
    "        if predictions[p] < 0:\n",
    "            predictions[p] = np.array(X_test1['user_min'])[p]\n",
    "    \n",
    "    print regr.coef_\n",
    "\n",
    "    MAE.append(sum(abs(Y_test - predictions))/len(Y_test))\n",
    "    kfold_predictions.append(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161.94307891326221, 161.04739308964915, 161.12305526194345, 160.71741622877846, 160.77141202351112]\n",
      "161.120471103\n"
     ]
    }
   ],
   "source": [
    "print MAE\n",
    "print np.mean(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# REGRESSION\n",
    "\n",
    "subdf = train_R[['artist', 'plays']]\n",
    "artist_medians = subdf.groupby('artist').median()\n",
    "artist_medians['medplays'] = artist_medians['plays']\n",
    "del artist_medians['plays']\n",
    "artist_medians = pd.DataFrame.to_dict(artist_medians)['medplays']\n",
    "\n",
    "subdf = train_R[['user', 'plays']]\n",
    "user_medians = subdf.groupby('user').median()\n",
    "user_medians['medplays'] = user_medians['plays']\n",
    "del user_medians['plays']\n",
    "user_medians = pd.DataFrame.to_dict(user_medians)['medplays']\n",
    "\n",
    "subdf = train_R[['user', 'plays']]\n",
    "user_mins = subdf.groupby('user').min()\n",
    "user_mins['medplays'] = user_mins['plays']\n",
    "del user_mins['plays']\n",
    "user_mins = pd.DataFrame.to_dict(user_mins)['medplays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "user_m = []\n",
    "artist_m = []\n",
    "user_min = []\n",
    "for i, j in zip(train_R['user'], train_R['artist']):\n",
    "    user_m.append(user_medians[i])\n",
    "    artist_m.append(artist_medians[j])\n",
    "    user_min.append(artist_medians[j])\n",
    "    \n",
    "train_R['user_median'] = user_m\n",
    "train_R['artist_median'] = artist_m\n",
    "train_R['user_min'] = user_min\n",
    "\n",
    "train_R1 = train_R\n",
    "\n",
    "\n",
    "# Test Dataset\n",
    "user_m = []\n",
    "artist_m = []\n",
    "user_min = []\n",
    "for i, j in zip(test['user'], test['artist']):\n",
    "    user_m.append(user_medians[i])\n",
    "    artist_m.append(artist_medians[j])\n",
    "    user_min.append(artist_medians[j])\n",
    "    \n",
    "test_R = test\n",
    "test_R['user_median'] = user_m\n",
    "test_R['artist_median'] = artist_m\n",
    "test_R['user_min'] = user_min\n",
    "\n",
    "test_R1 = test_R\n",
    "\n",
    "# Make X_train, X_test, Y_train\n",
    "X_train = train_R[['user_median', 'artist_median']]\n",
    "Y_train = train_R['plays']\n",
    "X_test = test_R[['user_median', 'artist_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.36811555  1.07681351]\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train)\n",
    "predictions = regr.predict(X_test)\n",
    "\n",
    "print regr.coef_\n",
    "\n",
    "for p in range(len(predictions)):\n",
    "    if predictions[p] < 0:\n",
    "        predictions[p] = np.array(test_R1['user_min'])[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 173.55398645,  200.69731239,  317.18502178, ...,  263.52717276,\n",
       "        367.44367985,  133.05394673])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write out test solutions - MEDIAN\n",
    "soln_file  = 'regression1.csv'\n",
    "test_file  = 'test.csv'\n",
    "\n",
    "with open(test_file, 'r') as test_fh:\n",
    "    test_csv = csv.reader(test_fh, delimiter=',', quotechar='\"')\n",
    "    next(test_csv, None)\n",
    "\n",
    "    with open(soln_file, 'w') as soln_fh:\n",
    "        soln_csv = csv.writer(soln_fh,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "        soln_csv.writerow(['Id', 'plays'])\n",
    "\n",
    "        for row in test_csv:\n",
    "            id     = row[0]\n",
    "            p = predictions[int(id)-1]\n",
    "\n",
    "            soln_csv.writerow([id, p])\n",
    "# KAGGLE SCORE: 161.34539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MEDIAN\n",
    "\n",
    "kf = KFold(n=len(train), n_folds=5, shuffle=False, random_state=3)\n",
    "kfold_predictions = []\n",
    "MAE = []\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    # Dataset for Regression\n",
    "    train_R = train_KM\n",
    "\n",
    "    X_train, X_test = train_R.ix[train_index, :], train_R.ix[test_index, :]\n",
    "    \n",
    "    subdf = X_train[['artist', 'plays']]\n",
    "    artist_medians = subdf.groupby('artist').median()\n",
    "    artist_medians['medplays'] = artist_medians['plays']\n",
    "    del artist_medians['plays']\n",
    "    artist_medians = pd.DataFrame.to_dict(artist_medians)['medplays']\n",
    "\n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    user_medians = subdf.groupby('user').median()\n",
    "    user_medians['medplays'] = user_medians['plays']\n",
    "    del user_medians['plays']\n",
    "    user_medians = pd.DataFrame.to_dict(user_medians)['medplays']\n",
    "    \n",
    "    subdf = X_train[['user', 'plays']]\n",
    "    user_mins = subdf.groupby('user').min()\n",
    "    user_mins['medplays'] = user_mins['plays']\n",
    "    del user_mins['plays']\n",
    "    user_mins = pd.DataFrame.to_dict(user_mins)['medplays']\n",
    "    \n",
    "    # Training Dataset\n",
    "    user_m = []\n",
    "    artist_m = []\n",
    "    user_min = []\n",
    "    for i, j in zip(X_train['user'], X_train['artist']):\n",
    "        try:\n",
    "            user_m.append(user_medians[i])\n",
    "        except:\n",
    "            user_m.append(0)\n",
    "        try:\n",
    "            user_min.append(user_mins[i])\n",
    "        except:\n",
    "            user_min.append(0)\n",
    "        artist_m.append(artist_medians[j])      \n",
    "\n",
    "    X_train['user_median'] = user_m\n",
    "    X_train['artist_median'] = artist_m\n",
    "    X_train['user_min'] = user_min\n",
    "    \n",
    "    X_train1 = X_train\n",
    "\n",
    "\n",
    "    # Test Dataset\n",
    "    user_m = []\n",
    "    artist_m = []\n",
    "    user_min = []\n",
    "    for i, j in zip(X_test['user'], X_test['artist']):\n",
    "        try:\n",
    "            user_m.append(user_medians[i])\n",
    "        except:\n",
    "            user_m.append(0)\n",
    "        try:\n",
    "            user_min.append(user_mins[i])\n",
    "        except:\n",
    "            user_min.append(0)\n",
    "        artist_m.append(artist_medians[j])\n",
    "\n",
    "    X_test['user_median'] = user_m\n",
    "    X_test['artist_median'] = artist_m\n",
    "    X_test['user_min'] = user_min\n",
    "    \n",
    "    X_test1 = X_test\n",
    "\n",
    "    # Make X_train, X_test, Y_train\n",
    "    Y_train = X_train['plays']\n",
    "    X_train = X_train[['user_median', 'artist_median']]\n",
    "    Y_test = X_test['plays']\n",
    "    X_test = X_test[['user_median', 'artist_median']]\n",
    "\n",
    "    # Predictions\n",
    "    ridge_regr = linear_model.Ridge(alpha=10, fit_intercept=True)\n",
    "    ridge_regr.fit(X_train, Y_train)\n",
    "    predictions = ridge_regr.predict(X_test)\n",
    "    \n",
    "    for p in range(len(predictions)):\n",
    "        if predictions[p] < 0:\n",
    "            predictions[p] = np.array(test_R1['user_min'])[p]\n",
    "\n",
    "    MAE.append(sum(abs(Y_test - predictions))/len(Y_test))\n",
    "    kfold_predictions.append(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165.23366052719109, 164.4411606532432, 164.45699510820177, 164.02331745432804, 164.14289619652567]\n",
      "164.459605988\n"
     ]
    }
   ],
   "source": [
    "print MAE # alpha = 10 Median;\n",
    "print np.mean(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165.23366053966097, 164.44116066578425, 164.45699512055506, 164.02331746702006, 164.1428962090848]\n",
      "164.459606\n"
     ]
    }
   ],
   "source": [
    "print MAE # alpha = 1 Median;\n",
    "print np.mean(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "user_m = []\n",
    "artist_m = []\n",
    "user_min = []\n",
    "for i, j in zip(train_R['user'], train_R['artist']):\n",
    "    user_m.append(user_medians[i])\n",
    "    artist_m.append(artist_medians[j])\n",
    "    user_min.append(artist_medians[j])\n",
    "    \n",
    "train_R['user_median'] = user_m\n",
    "train_R['artist_median'] = artist_m\n",
    "train_R['user_min'] = user_min\n",
    "\n",
    "train_R1 = train_R\n",
    "\n",
    "\n",
    "# Test Dataset\n",
    "user_m = []\n",
    "artist_m = []\n",
    "user_min = []\n",
    "for i, j in zip(test['user'], test['artist']):\n",
    "    user_m.append(user_medians[i])\n",
    "    artist_m.append(artist_medians[j])\n",
    "    user_min.append(artist_medians[j])\n",
    "    \n",
    "test_R = test\n",
    "test_R['user_median'] = user_m\n",
    "test_R['artist_median'] = artist_m\n",
    "test_R['user_min'] = user_min\n",
    "\n",
    "test_R1 = test_R\n",
    "\n",
    "# Make X_train, X_test, Y_train\n",
    "X_train = train_R[['user_median', 'artist_median']]\n",
    "Y_train = train_R['plays']\n",
    "X_test = test_R[['user_median', 'artist_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.32484819  1.09451686]\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.Ridge(alpha = 1)\n",
    "regr.fit(X_train, Y_train)\n",
    "predictions = regr.predict(X_test)\n",
    "\n",
    "print regr.coef_\n",
    "\n",
    "for p in range(len(predictions)):\n",
    "    if predictions[p] < 0:\n",
    "        predictions[p] = np.array(test_R1['user_min'])[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 173.02151765,  187.56096094,  313.16440966, ...,  271.00314363,\n",
       "        380.385284  ,  132.8707769 ])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write out test solutions - MEDIAN\n",
    "soln_file  = 'ridge_regression.csv'\n",
    "test_file  = 'test.csv'\n",
    "\n",
    "with open(test_file, 'r') as test_fh:\n",
    "    test_csv = csv.reader(test_fh, delimiter=',', quotechar='\"')\n",
    "    next(test_csv, None)\n",
    "\n",
    "    with open(soln_file, 'w') as soln_fh:\n",
    "        soln_csv = csv.writer(soln_fh,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "        soln_csv.writerow(['Id', 'plays'])\n",
    "\n",
    "        for row in test_csv:\n",
    "            id     = row[0]\n",
    "            p = predictions[int(id)-1]\n",
    "\n",
    "            soln_csv.writerow([id, p])\n",
    "# KAGGLE SCORE: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
